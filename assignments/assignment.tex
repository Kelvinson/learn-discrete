% No 'submit' option for the problems by themselves.
%\documentclass{harvardml}
% Use the 'submit' option when you submit your solutions.
\documentclass{harvardml}

% Put in your full name and email address.
\name{Your Name}
\email{email@fas.harvard.edu}

% You don't need to change these.
\assignment{\hspace{-3cm} Discrete Variables and Gradient Estimators}
\duedate{TBD}

\usepackage{url, enumitem}
\usepackage{amsfonts, amsmath}
\usepackage{listings}

% Some useful macros.
\def\ie{i.e.\ }
\def\eg{e.g.\ }
\def\iid{i.i.d.\ }
\def\simiid{\sim_{\mbox{\tiny iid}}}

\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\distNorm}{\mathcal{N}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\ident}{\mathbb{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\ep}{\varepsilon}
\newcommand{\Dir}{\text{Dirichlet}}

\newcommand{\vtheta}{\mathbf{\theta}}
\newcommand{\relaxed}{r}
\newcommand{\controlf}{c}  % Control variate for functions
\newcommand{\controlg}{\hat g}  % Control variate for gradients
\newcommand{\discreteDist}{p(b|\theta)}
\newcommand{\loss}{f(b)}
\newcommand{\lossGrad}{\loss{} \frac{\partial}{\partial \theta }\log \discreteDist{}}
\newcommand{\mcGrad}{\hat{g}}
\newcommand{\expectedLoss}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \loss{} \right]}
\newcommand{\expectedLossLogTrick}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \lossGrad{} \right]}
\newcommand{\var}{\mathbb{V}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\LL}[1]{\frac{\partial \log \pi(a_{#1}| s_{#1}, \theta)}{\partial \theta}}
\newcommand{\PT}{\frac{\partial}{\partial \theta}}
\newcommand{\PP}[1]{\frac{\partial}{\partial #1}}
\newcommand{\PPH}{\frac{\partial}{\partial \phi}}
\newcommand{\LP}[1]{\PT \log p(#1)}
\newcommand{\LZ}[1]{\frac{\log \pi(z_{#1}| s_{#1}, \theta)}{\partial \theta}}

\newcommand{\LAX}{{\textnormal{LAX}}}
\newcommand{\DLAX}{{\textnormal{DLAX}}}
\newcommand{\RL}{{\textnormal{RL}}}
\newcommand{\RELAX}{{\textnormal{RELAX}}}
\newcommand{\BAR}{{\textnormal{BAR}}}
\newcommand{\REBAR}{{\textnormal{REBAR}}}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}


\begin{document}
\pagenumbering{gobble}

This assignment is designed to get you comfortable deriving gradient estimators, and optimizing distributions over discrete random variables.

For most questions, the answers should be a few lines of derivations.
Throughout, you can assume that $p(b|\theta)$ is differentiable w.r.t. $\theta$.


\begin{problem}[Unbiasedness of the score-function estimator, 8 points]

Gradient estimators are simply functions that take a parameter vector $\theta$ and a (possibly stochastic) function $L(\theta)$, and return another vector $\hat g$ of the same size as $\theta$.
Gradient estimators are generally more useful the closer their estimate is to the true gradient, i.e.\ when $\hat g$ is close to $\PT L(\theta)$.

All else being equal, it's useful for a gradient estimator to be unbiased.
The unbiasedness of a gradient estimator guarantees that, if we decay the step size and run stochastic gradient descent for long enough (see Robbins \& Monroe), it will converge to a local optimum.

The standard REINFORCE, or score-function estimator is defined as:
%
\begin{align}
\hat g_\textnormal{SF}[f] = f \left( b \right) \PT \log p(b | \theta), \qquad b \sim p(b | \theta)
\end{align}

\begin{enumerate}[label=(\alph*)]
\item {\bf [2 points]} First, let's warm up with the score function.  Prove that the score function has zero expectation, i.e.\ $\E_{p(x|\theta)} \left[ \nabla_\theta \log p(x|\theta) \right] = 0$.
Assume that you can swap the derivative and integral operators.

\item {\bf [2 points]} Show that $\E_{p(b|\theta)} \left[ f(b) \PT \log p(b | \theta) \right] = \PT \E_{p(b|\theta)} \left[ f(b) \right]$.

\item {\bf [2 points]} Show that $\E_{p(b|\theta)} \left[ [f(b) - c] \PT \log p(b | \theta) \right] = \PT \E_{p(b|\theta)} \left[ f(b) \right]$ for any fixed $c$.

\item {\bf [2 points]} If the baseline depends on $b$, then REINFORCE will in general give biased gradient estimates.
Give an example where $\E_{p(b|\theta)} \left[ [f(b) - c(b)] \PT \log p(b | \theta) \right] \neq \PT \E_{p(b|\theta)}\left[ f(b) \right]$ for some function $c(b)$, and show that it is biased.
\end{enumerate}

The takeaway is that you can use a baseline to reduce the variance of REINFORCE, but not one that depends on the current action.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{problem}[Comparing variances of gradient estimators, 16 points]

If we restrict ourselves to consider only unbiased gradient estimators, then the main (and perhaps only) property we need to worry about is the variance of our estimators.
In general, optimizing with a lower-variance unbiased estimator will converge faster than a high-variance unbiased estimator.
However, which estimator has the lowest variance can depend on the function being optimized.
In this question, we'll look at which gradient estimators scale to large numbers of parameters, by computing their variance as a function of dimension.

For simplicity, we'll consider a toy problem.
The goal will be to estimate gradients of the expectation of a sum of one-dimensional Gaussians.
Each Gaussian has unit variance, and its mean is given by an element of the $D$-dimensional parameter vector $\theta$:
%
\begin{align}
f(\bx) & = \sum_{d=1}^D x_d \\
L(\theta) & = \E_{\bx \sim p(x|\theta)} \left[ f(\bx) \right]
= \E_{\bx \sim \distNorm(\theta, I)} \left[ \sum_{d=1}^D x_d \right]
\end{align}



\begin{enumerate}[label=(\alph*)]
\item {\bf [2 points]} As a warm-up, compute the variance of a single-sample simple Monte Carlo estimator of the objective $L(\theta)$:
%
\begin{align}
\hat L_{MC} 
= \sum_{d=1}^D x_d, \qquad \textnormal{where each } x_d \simiid \distNorm(\theta_d, 1)
\end{align}
%
That is, compute $\var \left[ \hat L_{MC} \right]$ as a function of $D$.

\item {\bf [2 points]}
Recall the definition of the score-function, or REINFORCE estimator:
%
\begin{align}
\hat g^\textnormal{SF}[f] = \left[f(x) - c(\theta) \right] \PT \log p(x | \theta), \qquad x \sim p(x | \theta)
\end{align}
%
Derive a closed form for this gradient estimator for the objective defined above as a deterministic function of $\epsilon$, a $D$-dimensional vector of standard normals.
Set the baseline to $c(\theta) = \sum_{d=1}^D \theta_d$.

\item {\bf [4 points]} Derive the variance of the above gradient estimator.
Because gradients are $D$-dimensional vectors, their covariance is a $D \times D$ matrix.
To make things easier, we'll consider only the variance of the gradient with respect to the first element of the parameter vector, $\theta_1$.
That is, derive the scalar value $\var \left[ \hat g^\textnormal{SF}_1 \right]$ as a function of $D$.
%Hint 1: Separate the terms with $\epsilon_1$ in them.
Hint: The third moment of a standard normal is 0, and the fourth moment is 3.
As a sanity check, consider the case where $D = 1$.

\item {\bf [4 points]} Next, let's look at the gold standard of gradient estimators, the reparameterization gradient estimator, where we reparameterize $x = T(\theta, \epsilon)$:
%
\begin{align}
\hat g^\textnormal{REPARAM}[f] = \frac{\partial f}{\partial x} \frac{\partial x}{\partial \theta}, \qquad \epsilon \sim p(\epsilon)
\end{align}
%
In this case, we can use the reparameterization $x = \theta + \epsilon$, with $\epsilon \sim \distNorm(0, I)$.
%
Derive this gradient estimator, and give $\var \left[ \hat g^\textnormal{REPARAM}_1 \right]$ as a function of $D$.

\item {\bf [2 points]} Finally, let's consider a more exotic gradient estimator, Evolution Strategies.
Following [Salimans et. al., 2016], we'll choose a meta-policy $p(\theta | \psi) = \distNorm(\theta | \psi, \sigma^2 I)$.
In this case, $\hat g^\textnormal{ES}$ estimates $\frac{\partial}{\partial \psi}\E_{p(\theta|\psi, \sigma)}\left[ L(\theta) \right]$, where $\psi$ specifies the mean of the distribution over $\theta$.
%
\begin{align}
\hat g^\textnormal{ES}[f]
& = \left[f(x) - c(\psi)\right] \frac{\partial}{\partial \psi} \log p(\theta | \psi, \sigma), 
& \qquad x \sim p(x | \theta),\quad \theta \sim p(\theta | \psi, \sigma) \\
& = \left[f(x) - c(\psi)\right] \frac{\partial}{\partial \psi} \log \distNorm(\theta | \psi, \sigma^2 I)
& \qquad x \sim \distNorm(x | \theta, I), \quad \theta \sim \distNorm(\theta | \psi, \sigma^2 I)
\end{align}
%
Set the baseline to $c(\psi) = \sum_{d=1}^D \psi_d$.
Derive a closed form for this gradient estimator as a deterministic function of $\sigma$ and two $D$-dimensional vectors of standard normals, $\epsilon_x$ and $\epsilon_\theta$.

\item {\bf [2 points]} Compute $\var \left[ \hat g^\textnormal{ES}_1 \right]$ as a function of $D$ and $\sigma$.
\end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{problem}[Sometimes stochastic policies are necessarily suboptimal, 8 points]

In reinforcement learning and hard attention models, we often parameterize a policy to stochastically choose actions according to the current state.
However, in many situations, the best policy is necessarily deterministic.

Given the objective
%
\begin{align}
L(\theta) = \E_{p(b|\theta)} \left[ f(b) \right]
\end{align}
%
where $b$ is a Bernoulli random variable,

\begin{enumerate}[label=(\alph*)]
\item {\bf [4 points]} Show that for any $f$ where $f(0) \neq f(1)$, that $L(\theta)$ has local optima only for deterministic policies, i.e.\ values of $\theta = 0$ or $\theta = 1$.  Proof by diagram is fine.

\item {\bf [4 points]} Show that if $L(\theta) = \E_{p(b|\theta)} \left[ f(b, \theta) \right]$, give an example function $f$ where a non-deterministic policy is optimal.
Again, proof by diagram is fine.
\end{enumerate}

The takeaway is that, for a fully-observed state in a non-adversarial setting, the only reason to use a stochastic policy is to make optimization easier.
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\begin{problem}[Representing and computing Categorical distributions, 8 points]

There are several ways to parameterization Categorical distributions.

\begin{enumerate}[label=(\alph*)]
\item {\bf [1 point]} If we parameterize a $D$-dimensional categorial distribution using a $D$-dimensional vector $\theta$ as
%
\begin{align}
p(c | \theta) = \theta_c
\end{align}
%
what is the allowable range of the vector $\theta$? Or, answer this question: does setting all $\theta_c = 0$ result in a valid probability distribution over $c$?

\item {\bf [1 point]} One problem with this parameterization is that it is hard to represent very small probabilities.
If we parameterize a $D$-dimensional categorial distribution as
%
\begin{align}
\log p(c | \theta) = \theta_c
\end{align}
%
what is the allowable range of the vector $\theta$? Or, answer this question: does setting all $\theta_c = 0$ result in a valid probability distribution over $c$?

What is the computational cost of evaluating $\log p(c | \theta)$, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Another problem is that optimizing a point in the simplex requires using a constrained optimization routine.

If we parameterize a $D$-dimensional categorial distribution as
%
\begin{align}
\log p(c | \theta) = \theta_c - \log \sum_{c' = 1}^D \exp \theta_{c'}
\end{align}
%
what is the allowable range of the vector $\theta$? Or: answer this question: does setting all $\theta_c = 0$ result in a valid probability distribution over $c$?
What is the computational cost of evaluating $\log p(c | \theta)$, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Using the parameterization
%
\begin{align}
\log p(c | \theta) = \theta_c - \log \sum_{c' = 1}^D \exp \theta_{c'}
\end{align}
%
write the gradient of $\log p(c | \theta)$ w.r.t.\ the vector $\theta$.
What is the computational complexity of evaluating this gradient, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Now let's consider the analogous problem of parameterizing a Bernoulli random variable.
Give a monotonic, numerically stable paramaterization for $\log p(b | \theta)$ such that all $\theta \in \mathbb{R}$ give valid probabilities, and there is a one-to-one correspondence between $\theta$ and $p(b)$.
Hint: One good approach is to use the \texttt{logsumexp} trick.
\end{enumerate}

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{problem}[BONUS: Optimal surrogates. 15 points]

Warning: These questions are probably hard and time-consuming.  I don't know the answers and I don't expect more than one or two people to get them. For details on $p(z|b, \theta)$, see the REBAR or RELAX papers.

Consider the objective
%
\begin{align}
L(\theta) 
= \E_{p(b|\theta)} \left[ f(b) \right]
= \E_{p(b|\theta)} \left[ (b - t)^2 \right]
\end{align}
%
where $b$ is a single Bernoulli random variable.


\begin{enumerate}[label=(\alph*)]
\item {\bf [Hard 5 points]} The REBAR estimator is given by:
\begin{align}
\hat g_\textnormal{REBAR} = \left[ f(b) - \eta f(\sigma_\lambda(\tilde{z})) \right] \PT \log p(b|\theta) + \PT \eta f(\sigma_\lambda(z)) - \PT \eta f(\sigma_\lambda(\tilde{z})) \\
\qquad b = H(z), z \sim p(z|\theta), \tilde{z} \sim p(z|b, \theta) \nonumber
\end{align}
%
For this objective, find the optimal (as in lowest variance) temperature $\lambda$ and scale $\eta$ for REBAR as a function of $t$ and $\theta$.
\item {\bf [Harder, 5 points]} The discrete version of the \LAX{} estimator is given by:
%
\begin{align}
\label{eq:discrete lax}
\hat g_\DLAX = f(b) \PT \log p(b|\theta) - c_\phi(z) \PT \log p(z|\theta) + \PT c_\phi(z), \qquad b = H(z), z \sim p(z|\theta).
\end{align}
%
Find the optimal surrogate $c_\phi(z)$ for DLAX as a function of $t$ and $\theta$.
Compute the variance of this estimator as a function of $t$ and $\theta$.
\item {\bf [Hardest, 5 points]} %
\begin{align}
\hat g_\textnormal{RELAX} = \left[ f(b) - c_\phi(\tilde{z}) \right] \PT \log p(b|\theta) + \PT c_\phi(z) - \PT c_\phi (\tilde{z}) \\
\qquad b = H(z), z \sim p(z|\theta), \tilde{z} \sim p(z|b, \theta) \nonumber
\end{align}
%
Find the optimal surrogate $c_\phi(z)$ for RELAX as a function of $t$ and $\theta$.
Compute the variance of this estimator as a function of $t$ and $\theta$.
\end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\end{document}