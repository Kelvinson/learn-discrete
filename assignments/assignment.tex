% No 'submit' option for the problems by themselves.
%\documentclass{harvardml}
% Use the 'submit' option when you submit your solutions.
\documentclass{harvardml}

% Put in your full name and email address.
\name{Your Name}
\email{email@fas.harvard.edu}

% You don't need to change these.
\assignment{\hspace{-3cm} Discrete Variables and Gradient Estimators}
\duedate{TBD}

\usepackage{url, enumitem}
\usepackage{amsfonts, amsmath}
\usepackage{listings}

% Some useful macros.
\newcommand{\given}{\,|\,}
\newcommand{\R}{\mathbb{R}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\trans}{\mathsf{T}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\distNorm}{\mathcal{N}}
\newcommand{\bzero}{\mathbf{0}}
\newcommand{\ident}{\mathbb{I}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\ep}{\varepsilon}
\newcommand{\Dir}{\text{Dirichlet}}

\newcommand{\vtheta}{\mathbf{\theta}}
\newcommand{\relaxed}{r}
\newcommand{\controlf}{c}  % Control variate for functions
\newcommand{\controlg}{\hat g}  % Control variate for gradients
\newcommand{\discreteDist}{p(b|\theta)}
\newcommand{\loss}{f(b)}
\newcommand{\lossGrad}{\loss{} \frac{\partial}{\partial \theta }\log \discreteDist{}}
\newcommand{\mcGrad}{\hat{g}}
\newcommand{\expectedLoss}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \loss{} \right]}
\newcommand{\expectedLossLogTrick}{\mathbb{E}_{\discreteDist{}} \! \left[ \, \lossGrad{} \right]}
\newcommand{\var}{\mathbb{V}}
\newcommand{\vu}{\mathbf{u}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\LL}[1]{\frac{\partial \log \pi(a_{#1}| s_{#1}, \theta)}{\partial \theta}}
\newcommand{\PT}{\frac{\partial}{\partial \theta}}
\newcommand{\PP}[1]{\frac{\partial}{\partial #1}}
\newcommand{\PPH}{\frac{\partial}{\partial \phi}}
\newcommand{\LP}[1]{\PT \log p(#1)}
\newcommand{\LZ}[1]{\frac{\log \pi(z_{#1}| s_{#1}, \theta)}{\partial \theta}}

\newcommand{\LAX}{{\textnormal{LAX}}}
\newcommand{\DLAX}{{\textnormal{DLAX}}}
\newcommand{\RL}{{\textnormal{RL}}}
\newcommand{\RELAX}{{\textnormal{RELAX}}}
\newcommand{\BAR}{{\textnormal{BAR}}}
\newcommand{\REBAR}{{\textnormal{REBAR}}}

\theoremstyle{plain}
\newtheorem{lemma}{Lemma}


\begin{document}

This assignment is designed to get you comfortable deriving gradient estimators, and optimizing distributions over discrete random variables.

For most questions, the answers should be a few lines of derivations.
Throughout, you can assume that $p(b|\theta)$ is differentiable w.r.t. $\theta$.


\begin{problem}[Unbiasedness of the score-function estimator, 10 points]

Gradient estimators are simply functions that take a parameter vector $\theta$ and a (possibly stochastic) function $L(\theta)$, and return another vector $\hat g$ of the same size as $\theta$.
Gradient estimators are generally more useful the closer their estimate is to the true gradient, i.e.\ when $\hat g$ is close to $\PT L(\theta)$.

All else being equal, it's useful for a gradient estimator to be unbiased.
The unbiasedness of a gradient estimator guarantees that, if we decay the step size and run stochastic gradient descent for long enough (see Robbins \& Monroe), it will converge to a local optimum.

The standard REINFORCE, or score-function estimator is defined as:
%
\begin{align}
\hat g_\textnormal{SF}[f] = f \left( b \right) \PT \log p(b | \theta), \qquad b \sim p(b | \theta)
\end{align}
%
%This estimator is unbiased, but in general has high variance.
%Intuitively, this estimator is limited by the fact that it doesn't use any information about how $f$ depends on $b$, only on the final outcome $f(b)$.
%
\begin{enumerate}[label=(\alph*)]
\item {\bf [2 points]} First, let's warm up with the score function.  Prove that the score function has zero expectation, i.e.\ $\E_{p(x|\theta)} \left[ \nabla_\theta \log p(x|\theta) \right] = 0$.
Hint: Under some conditions, one can swap the derivative and integral operators.

\item {\bf [2 points]} Show that $\E_{p(b|\theta)} \left[ f(b) \PT \log p(b | \theta) \right] = \PT \E_{p(b|\theta)} \left[ f(b) \right]$.

\item {\bf [2 points]} Show that $\E_{p(b|\theta)} \left[ [f(b) - c] \PT \log p(b | \theta) \right] = \PT \E_{p(b|\theta)} \left[ f(b) \right]$ for any fixed $c$.

\item {\bf [2 points]} Derive the variance-minimizing value of $c$.

\item {\bf [2 points]} If the baseline depends on $b$, then REINFORCE will in general give biased gradient estimates.
Give an example where $\E_{p(b|\theta)} \left[ [f(b) - c(b)] \PT \log p(b | \theta) \right] \neq \PT \E_{p(b|\theta)}\left[ f(b) \right]$ for some function $c(b)$, and show that it is biased.
\end{enumerate}

The takeaway is that you can use a baseline to reduce the variance of REINFORCE, but not one that depends on the current action.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{problem}[Comparing variances of gradient estimators, 14 points]

If we restrict ourselves to consider only unbiased gradient estimators, then the main (and perhaps only) property we need to worry about is the variance of our estimators.
In general, optimizing with a lower-variance unbiased estimator will converge faster than a high-variance unbiased estimator.
However, which estimator has the lowest variance can depend on the function being optimized.

In this question, we'll look at which gradient estimators scale to large numbers of parameters, by computing their variance as a function of dimension.

For simplicity, we'll consider a toy problem.
The goal will be to estimate gradients of a sum of one-dimensional Gaussians with means given by a $D$-dimensional parameter vector $\theta$ and unit variance:
%
\begin{align}
L(\theta) 
= \E_{\bx \sim \distNorm(\theta, I)} \left[ f(\bx) \right]
= \E_{\bx \sim \distNorm(\theta, I)} \left[ \sum_{d=1}^D x_d \right]
\end{align}



\begin{enumerate}[label=(\alph*)]
\item {\bf [2 points]} As a warm-up, let's compute the variance of a single-sample simple Monte Carlo estimator of the objective $L(\theta)$:
%
\begin{align}
\hat L_{MC} 
= \sum_{d=1}^D x_d, \qquad \textnormal{where each } x_d \sim \distNorm(\theta_d, 1)
\end{align}
%
That is, compute $\var \left[ \hat L_{MC} \right]$ as a function of $D$.

\item {\bf [2 points]} Next, we'll compare the variance of different gradient estimators.

Recall the definition of the score-function, or REINFORCE estimator:
%
\begin{align}
\hat g^\textnormal{SF}[f] = f(x) \PT \log p(x | \theta), \qquad x \sim p(x | \theta)
\end{align}
%

Because gradients are $D$-dimensional vectors, they actually have a $D \times D$ covariance matrix.
For this question, we'll consider the variance of the gradient with respect to the first element of the parameter vector, $\theta_1$.
%
That is, derive $\var \left[ \hat g^\textnormal{SF}_1 \right]$ as a function of $D$.

\item {\bf [4 points]} Next, let's look at the gold standard of gradient estimators, the reparameterization gradient estimator, where we reparameterize $x = T(\theta, \epsilon)$:
%
\begin{align}
\hat g^\textnormal{REPARAM}[f] = \frac{\partial f}{\partial x} \frac{\partial x}{\partial \theta}, \qquad \epsilon \sim p(\epsilon)
\end{align}
%
In this case, we can use the reparameterization $x = \theta + \epsilon$, with $\epsilon \sim \distNorm(0, 1)$.

Derive $\var \left[ \hat g^\textnormal{REPARAM}_1 \right]$ as a function of $D$.

\item {\bf [6 points]} Finally, let's consider a more exotic gradient estimator, Evolution Strategies.
Following [Salimans et. al., 2016], we'll choose a meta-policy $p(\hat \theta | \psi) = \distNorm(\hat \theta |  \theta, \sigma I)$
%
\begin{align}
\hat g^\textnormal{ES}[f]
& = f(x) \PT \log p(\hat \theta | \psi), 
& \qquad x \sim p(x | \hat \theta),\quad \hat \theta \sim p(\hat \theta | \psi) \\
& = f(x) \PT \log \distNorm(\hat \theta | \theta, \sigma I)
& \qquad x \sim \distNorm(x | \hat \theta, I), \quad \hat \theta \sim \distNorm(\hat \theta | \theta, \sigma I)
%& = f(\theta +  \epsilon_\theta \sigma + \epsilon_x) \PT \log \distNorm(\theta + \epsilon_\theta \sigma | \theta, I),
%& \qquad \epsilon_x \sim \distNorm(0, I), \quad \epsilon_\theta \sim \distNorm(0, I) \\
%& = f(\theta + \sigma_\theta \epsilon_\theta + \sigma_x) \frac{\epsilon_\theta}{\sigma_\theta},
%& \qquad \epsilon_x \sim \distNorm(0, I), \quad \hat \epsilon_\theta \sim \distNorm(0, I)
\end{align}
%
Compute $\var \left[ \hat g^\textnormal{ES}_1 \right]$.
Hint 1: It is a function of both $\sum_{d=1}^D \theta_d$ and $\sigma$.
Hint 2: First, write down the gradient estimator w.r.t.\ $\theta_1$, reparameterizing all variables to be deterministic functions of standard normal distributions.
Hint 3: Check that its expectation is correct by comparing it against your answers above.
\end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\begin{problem}[The pointlessness of stochastic policies, 8 points]

In reinforcement learning and hard attention models, we often parameterize a policy to stochastically choose actions according to the current state.
However, in many situations, the best policy is necessarily deterministic.

Given the objective
%
\begin{align}
L(\theta) = \E_{p(b|\theta)} \left[ f(b) \right]
\end{align}
%
where $b$ is a Bernoulli random variable,

\begin{enumerate}[label=(\alph*)]
\item {\bf [4 points]} Show that for any $f$, $L(\theta)$ has local optima only for deterministic policies, i.e.\ values of $\theta = 0$ or $\theta = 1$.

\item {\bf [4 points]} Show that if $L(\theta) = \E_{p(b|\theta)} \left[ f(b, \theta) \right]$, give an example where a non-deterministic policy is optimal.
\end{enumerate}

The takeaway is that, for a fully-observed state in a non-adversarial setting, the only reason to use a stochastic policy is to make optimization easier.
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\begin{problem}[Representing and computing Categorical distributions, 8 points]

A Categorical distribution can be represented by a point in the simplex.

\begin{enumerate}[label=(\alph*)]
\item {\bf [1 point]} If we parameterize a $D$-dimensional categorial distribution as
%
\begin{align}
p(c | \theta) = \theta_c
\end{align}
%
what is the allowable range of the vector $\theta$?

\item {\bf [1 point]} One problem with this parameterization is that it is hard to represent very small probabilities.
If we parameterize a $D$-dimensional categorial distribution as
%
\begin{align}
\log p(c | \theta) = \theta_c
\end{align}
%
what is the allowable range of the vector $\theta$?
What is the computational cost of evaluating $\log p(c | \theta)$, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Another problem is that optimizing a point in the simplex requires using a constrained optimization routine.

If we parameterize a $D$-dimensional categorial distribution as
%
\begin{align}
\log p(c | \theta) = \theta_c - \log \sum_{c' = 1}^D \exp \theta_{c'}
\end{align}
%
what is the allowable range of the vector $\theta$? 
What is the computational cost of evaluating $\log p(c | \theta)$, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Using the parameterization
%
\begin{align}
\log p(c | \theta) = \theta_c - \log \sum_{c' = 1}^D \exp \theta_{c'}
\end{align}
%
write the gradient of $\log p(c | \theta)$ w.r.t.\ the vector $\theta$.
What is the computational complexity of evaluating this gradient, for a particular value of $c$, as a function of $D$?

\item {\bf [2 points]} Now let's consider the analogous problem of parameterizing a Bernoulli random variable.
Give a monotonic, numerically stable paramaterization for $\log p(b | \theta)$ such that all $\theta \in \mathbb{R}$ give valid probabilities, and there is a one-to-one correspondence between $\theta$ and $p(b)$.
\end{enumerate}

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%







\begin{problem}[Bonus: Optimal surrogates. 15 points]

Consider the objective
%
\begin{align}
L(\theta) 
= \E_{p(b|\theta)} \left[ f(b) \right]
= \E_{p(b|\theta)} \left[ (b - t)^2 \right]
\end{align}
%
where $b$ is a single Bernoulli random variable.


\begin{enumerate}[label=(\alph*)]
\item {\bf [Hard 5 points]} The REBAR estimator is given by:
\begin{align}
\hat g_\textnormal{REBAR} = \left[ f(b) - \eta f(\sigma_\lambda(\tilde{z})) \right] \PT \log p(b|\theta) + \PT \eta f(\sigma_\lambda(z)) - \PT \eta f(\sigma_\lambda(\tilde{z})) \\
\qquad b = H(z), z \sim p(z|\theta), \tilde{z} \sim p(z|b, \theta) \nonumber
\end{align}
%
For this objective, find the optimal temperature $\lambda$ and scale $\eta$ for REBAR as a function of $t$ and $\theta$.
%This requires only calculus.
\item {\bf [Harder, 5 points]} The discrete version of the \LAX{} estimator is given by:
%
\begin{align}
\label{eq:discrete lax}
\hat g_\DLAX = f(b) \PT \log p(b|\theta) - c_\phi(z) \PT \log p(z|\theta) + \PT c_\phi(z), \qquad b = H(z), z \sim p(z|\theta).
\end{align}
%
Find the optimal surrogate $c_\phi(z)$ for DLAX as a function of $t$ and $\theta$.
Compute the variance of this estimator as a function of $t$ and $\theta$.
%This will require variational calculus, or solving a PDE.
\item {\bf [Hardest, 5 points]} %
\begin{align}
\hat g_\textnormal{RELAX} = \left[ f(b) - c_\phi(\tilde{z}) \right] \PT \log p(b|\theta) + \PT c_\phi(z) - \PT c_\phi (\tilde{z}) \\
\qquad b = H(z), z \sim p(z|\theta), \tilde{z} \sim p(z|b, \theta) \nonumber
\end{align}
%
Find the optimal surrogate $c_\phi(z)$ for RELAX as a function of $t$ and $\theta$.
Compute the variance of this estimator as a function of $t$ and $\theta$.
\end{enumerate}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%\begin{problem}[Bonus: Optimal Gaussian reparameterization, 10 points]

%If $$X \sim \distNorm(\mu, \Sigma)$$, we can draw samples in a reparameterizable way by $$x = \mu + L \epsilon$$ where $\epsilon \sim \distNorm(0, I)$, and $L^T L = \Sigma$.

%\begin{enumerate}[label=(\alph*)]
%\item Given $\mu$ and $\Sigma$, what $L$ minimizes the trace of the variance of $\nabla_L X$?
%\end{enumerate}
%\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% You can write your solution here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




\end{document}